# ğŸº Ambev Data Platform (Terraform + GCP)

## ğŸš€ VisÃ£o Geral
A **Ambev Data Platform** foi desenvolvida para transformar **dados brutos de vendas e canais** em **informaÃ§Ãµes analÃ­ticas rÃ¡pidas, consistentes e confiÃ¡veis**.  
O projeto utiliza **Infraestrutura como CÃ³digo (IaC)** com **Terraform** para provisionar recursos no **Google Cloud Platform (GCP)**, orquestrando pipelines **ETL containerizados** em **Cloud Run Jobs** e armazenando resultados em **BigQuery**.

---

## ğŸ—ï¸ Arquitetura Geral

![Arquitetura GCP](https://upload.wikimedia.org/wikipedia/commons/5/5f/Google_Cloud_Platform_logo.svg)

```text
[Terraform]
   â†“ provisiona
[Cloud Storage] â†’ [Cloud Run Jobs] â†’ [BigQuery] â†’ [Looker Studio]
                       â†‘
     (Dockerfiles Bronze/Silver/Gold + Python ETLs)
```

### ğŸ”¹ Camadas de Dados

| Camada | DescriÃ§Ã£o | Script Principal |
|--------|------------|------------------|
| **Bronze** | IngestÃ£o e padronizaÃ§Ã£o dos dados brutos. | `bronze.py` |
| **Silver** | Limpeza, enriquecimento e junÃ§Ã£o das tabelas. | `silver.py` |
| **Gold** | AgregaÃ§Ãµes analÃ­ticas, KPIs e mÃ©tricas de negÃ³cio. | `gold.py` |

---

## âš™ï¸ Estrutura de DiretÃ³rios

```bash
infra/
â”œâ”€â”€ deploy.ps1
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.bronze
â”œâ”€â”€ Dockerfile.silver
â”œâ”€â”€ Dockerfile.gold
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bronze.py
â”‚   â”œâ”€â”€ silver.py
â”‚   â”œâ”€â”€ gold.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ abi_bus_case1_beverage_channel_group_20210726.csv
â”‚   â””â”€â”€ abi_bus_case1_beverage_sales_20210726.csv
â””â”€â”€ terraform/
    â”œâ”€â”€ bigquery.tf
    â”œâ”€â”€ cloudrun.tf
    â”œâ”€â”€ iam.tf
    â”œâ”€â”€ providers.tf
    â”œâ”€â”€ run.tf
    â”œâ”€â”€ storage.tf
    â”œâ”€â”€ variables.tf
    â”œâ”€â”€ terraform.tfstate
    â”œâ”€â”€ terraform.tfstate.backup
    â”œâ”€â”€ .terraform.lock.hcl
    â””â”€â”€ .terraform/
```

---

## ğŸŒ Recursos Provisionados (Terraform)

| Recurso | DescriÃ§Ã£o |
|----------|------------|
| **IAM** | CriaÃ§Ã£o de service accounts e permissÃµes granulares. |
| **Cloud Storage** | Buckets de ingestÃ£o e staging para datasets. |
| **BigQuery** | Datasets `abi_bronze`, `abi_silver`, `abi_gold`. |
| **Cloud Run Jobs** | ExecuÃ§Ã£o automatizada dos containers ETL. |

---

## ğŸ§© ETL Containers

Cada camada do pipeline possui um **Dockerfile** e um script dedicado:

| Container | Dockerfile | Script | Output |
|------------|-------------|---------|---------|
| Bronze | `Dockerfile.bronze` | `bronze.py` | Dados padronizados |
| Silver | `Dockerfile.silver` | `silver.py` | Dados limpos e enriquecidos |
| Gold | `Dockerfile.gold` | `gold.py` | KPIs e mÃ©tricas de negÃ³cio |

A execuÃ§Ã£o Ã© orquestrada via **Cloud Run Jobs**, conforme definido em `cloudrun.tf`.

---

## ğŸ“Š VisualizaÃ§Ã£o e Consumo
Os datasets Gold sÃ£o consumidos diretamente no **Looker Studio**, permitindo anÃ¡lises e dashboards sobre:
- Vendas por marca e canal  
- Market share por regiÃ£o  
- Crescimento mensal e sazonalidade  

---

## ğŸ§  Tecnologias Utilizadas

| Categoria | Ferramenta |
|------------|-------------|
| Infraestrutura | Terraform |
| Cloud Platform | Google Cloud Platform (GCP) |
| Processamento | Cloud Run Jobs |
| Armazenamento | Cloud Storage |
| Data Warehouse | BigQuery |
| VisualizaÃ§Ã£o | Looker Studio |
| Linguagem | Python 3 |
| DependÃªncias | Pandas, Google Cloud SDK |

---

## ğŸš€ Como Executar

### 1. Configurar variÃ¡veis de ambiente
```bash
export GOOGLE_CLOUD_PROJECT="ambev-data"
export GOOGLE_APPLICATION_CREDENTIALS="key.json"
```

### 2. Inicializar e aplicar o Terraform
```bash
cd terraform
terraform init
terraform apply -auto-approve
```

### 3. Enviar datasets para o Cloud Storage
ApÃ³s o Terraform criar o bucket (`ambev-beverage-mvp`), envie os arquivos para o GCS:

```bash
cd ../datasets
gsutil cp abi_bus_case1_beverage_channel_group_20210726.csv gs://ambev-beverage-mvp/raw/
gsutil cp abi_bus_case1_beverage_sales_20210726.csv gs://ambev-beverage-mvp/raw/
```

Esses arquivos serÃ£o utilizados pela camada **Bronze** como ponto de partida para o pipeline.

### 4. Build e push das imagens Docker
```bash
gcloud builds submit --tag gcr.io/ambev-data/etl-bronze .
gcloud builds submit --tag gcr.io/ambev-data/etl-silver .
gcloud builds submit --tag gcr.io/ambev-data/etl-gold .
```

### 5. Executar os jobs no Cloud Run
```bash
gcloud run jobs execute etl-bronze
gcloud run jobs execute etl-silver
gcloud run jobs execute etl-gold
```

---

## ğŸ§­ Roadmap Futuro
- IntegraÃ§Ã£o com **Cloud Composer (Airflow)** para agendamento centralizado.  
- Camada de **Data Quality** (Great Expectations / dbt tests).  
- Deploy automatizado via **GitHub Actions**.  
- Versionamento de dados com **BigQuery Time Travel**.  

---

## ğŸ‘¨â€ğŸ’» Autor
**Marcus Vinicius Mendes**  
_Data Engineer Manager â€“ Ambev (Projeto Educacional)_  
ğŸ“§ mvmendes96@gmail.com  
ğŸ“ Curitiba â€“ PR, Brasil  
